{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"training_linear_models\"\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)   \n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)   #affichage non tronqué"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recupere BDD -> Matrices d'inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ROWS=671\n",
    "\n",
    "#recupere les données du fichier csv et les mets dans les ndarray X et Y (grace à to_numpy(),sinon ce sont des types dataframe)\n",
    "#Ils ont donc une shape de (671,1)\n",
    "\n",
    "X= pd.read_csv(\"sentence.csv\", encoding = \"utf-8\",sep=\";\",usecols = [\"sen_sentence\"],nrows=ROWS).to_numpy()\n",
    "Y= pd.read_csv(\"sentence.csv\", encoding = \"utf-8\",sep=\";\",usecols = [\"sen_en_full\"],nrows=ROWS).to_numpy().reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint                            #permet un affichage ligne par ligne,équivalent au println() de java\n",
    "from nltk.tag import StanfordPOSTagger   #librairie utilisée pour tagger\n",
    "\n",
    "jar = './stanford-postagger-full-2018-10-16/stanford-postagger.jar'    # recupere les modeles telechargés dans le fichier\n",
    "model = './stanford-postagger-full-2018-10-16/models/french.tagger'\n",
    "java_path = \"C:/Program Files/Java/jre1.8.0_211/bin/java.exe\"          # on s'assure du lien pour java\n",
    "os.environ['JAVAHOME'] = java_path                                     # variable d'environnement\n",
    "\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding='utf-8' )          #prêt à être utilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonctions qui transforme une phrase en matrice lignes (taille 28) de nos inputs , binarisation des tags\n",
    "\n",
    "def senToTags(sentence):\n",
    "    sen_tags=[m[1] for m in pos_tagger.tag(sentence)]        #tout les tags presents dans la phrase\n",
    "    #dictionnaire des tags possibles (28)\n",
    "    tags=['P', 'N',  'ADJ', 'NC', 'DET', 'NPP', 'V', 'VPP', 'ADV', 'PROREL', 'CLS', 'VINF', 'CC', 'PUNC', 'PRO', 'ET', 'CS', 'CLR', 'CLO', 'VPR', 'ADVWH', 'C', 'VIMP', 'CL', 'VS', 'PROWH', 'ADJWH', 'PREF']\n",
    "    result=list()\n",
    "    for tag in tags:            # pour chaque tag du dictionnaire\n",
    "        if tag in sen_tags:     # on vérifie s'il est présent dans la phrase\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exemple : senToTags(\" Les Americains ont réagi en six semaines là où il a fallu neuf mois aux Francais.\") --->  [1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"exemple : senToTags(\\\" Les Americains ont réagi en six semaines là où il a fallu neuf mois aux Francais.\\\") ---> \",senToTags(\"Les Americains ont réagi en six semaines là où il a fallu neuf mois aux Francais.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time execution :  1425.9014670848846\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()          # recupere l'heure exacte pour calculer le temps d'execution\n",
    "X_tmp=list()                 #initialise une list\n",
    "for i in range(len(X)):\n",
    "    X_tmp+=senToTags(X[i])  #on applique cette fonction (cellule precedente) à chaque phrase, à chaque ligne de X donc\n",
    "end = time.time()\n",
    "print(\"time execution : \",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 346\n",
      "Before OverSampling, counts of label '0': 257 \n",
      "\n",
      "After OverSampling, the shape of train_X: (692, 28)\n",
      "After OverSampling, the shape of train_y: (692,) \n",
      "\n",
      "After OverSampling, counts of label '1': 346\n",
      "After OverSampling, counts of label '0': 346\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tmp=np.asarray(X_tmp).reshape(-1,28)  #redimensionne en imposant 28 colonnes , peut importe le nb de lignes\n",
    "#simple separation de X_tmp en X_train et X_test selon le ratio , test_size ... meme chose pour Y\n",
    "# le random_state=2 permet uniquement de fixer l'aleatoire apres un premier tirage pour ne pas avoir de problemes en reexecutant \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tmp, Y, test_size=0.1, random_state=2)\n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(Y_train==1)))    # nb de label=1 avant equilibrage\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(Y_train==0))) # nb de label=0 avant equilibrage\n",
    "\n",
    "\n",
    "sm = SMOTE(random_state=2)                       #initialise le regulateur qui va en realité crée des doublons de la classe\n",
    "X_train, Y_train = sm.fit_sample(X_train, Y_train)  # inferieur pour egaliser le nb de labels\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(Y_train.shape)) \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(Y_train == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(Y_train == 0))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions pour le reseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#couche de neurones, on aurait pu utilisé tf.layers.dense() qui prend les meme parametres et qui crée une couche intégralement connectée, elle se charge de creer les var necessaires en utilisant la stratégie d'initialisationj appropriée\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name): #portée de nom , facultatif, pour mieux retrouver\n",
    "        n_inputs = int(X.shape[1])  #nb entrées = nb de colonnes de X donc 28 \n",
    "        \n",
    "        #ces 3 lignes créent la var W qui contiendra la matrice des poids des connexions entre chaque entrée et chaque neurone\n",
    "        stddev = 2 / np.sqrt(n_inputs)  #facultatif, on tronquera avec cet ecart-type qui permet à l'algo de converger plus rapidement,plus stddev est grand et plus l'ecart sera grand\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev) #matrice n_inputs x n_neurons valeurs proche de 0 , tronquées avec stddeb   \n",
    "        W = tf.Variable(init,name=\"kernel\")  #weights random\n",
    "        \n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\") #biais,initialisé à 0\n",
    "        Z = tf.matmul(X, W) + b    #z=X.W +b\n",
    "        tf.cast(Z,tf.int32)\n",
    "        if activation is not None:  #passe par une fonction d'activation si elle existe\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "#return un Y avec une dimension = valeur max+1 , donc ici y_one_hot aura une shape=(any,2)\n",
    "#voir exemple cellule d'apres\n",
    "\n",
    "def to_one_hot(y):\n",
    "    n_classes = y.max() + 1  # dimension +1 dans notre cas , ce sera forcement 2\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m, n_classes)) #matrice de dimension(m,n_classes) remplit de 0\n",
    "    Y_one_hot[np.arange(m), y] = 1  #on mets un 1 dans la colonne 1 etc.. voir exemple\n",
    "    return Y_one_hot\n",
    "\n",
    "\n",
    "\n",
    "#Homogeneisation, decoupage de X et Y en paquets de taille = batch_size, choisis aléatoirement dans X et Y , permets d'eviter\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):               #de calculer que sur une série de données \"particuliere\" comme l'interview de Rihanna et pas sur le reste, ici au moins , on a des données homogenes\n",
    "    rnd_idx = np.random.permutation(len(X))              #array de taille = 671  de valeur aleatoires < 671  \n",
    "    n_batches = len(X) // batch_size               #nb de paquets\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):  #np.array_split(X,n) split X en n parties(array)\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]     #on declare nos paquets \n",
    "        yield X_batch, y_batch                           #yield est comme un return sauf que c'est un generateur , ce qui permet de gagner en memoire , il ne parcourt pas à chaque appel toute la fonction mais recupere à la volée la valeur puis incremente ensuite le generateur , un lien qui explique davanatage : https://www.journaldunet.fr/web-tech/developpement/1202863-a-quoi-sert-le-mot-cle-yield-en-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exemples fonctions utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_shape =  (6,)\n",
      "y1_shape =  (6, 2)\n",
      "y1 = [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#exemple y_one_hot\n",
    "\n",
    "y=np.asarray([1,0,1,1,0,0])\n",
    "\n",
    "print(\"y_shape = \",y.shape)\n",
    "y1=to_one_hot(y)\n",
    "print(\"y1_shape = \",y1.shape)\n",
    "\n",
    "#on mets pour chaque element de y ayant pour valeur i , un 1 à la colonne i et sur le reste de la ligne, que des 0 \n",
    "print(\"y1 =\" ,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25330153  0.15354085]\n",
      " [ 0.03800653  0.3326441 ]\n",
      " [-0.09796198  0.03484468]]\n"
     ]
    }
   ],
   "source": [
    "#exemple truncated_normal\n",
    "\n",
    "import tensorflow as tf\n",
    "with  tf.Session() as sess:\n",
    "    p=tf.truncated_normal((3,2),stddev=0.2).eval()\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [[10 11 12]\n",
      " [13 14 15]]\n",
      "axis=0 -> [1 1 1]\n",
      "axis=1 -> [2 2]\n"
     ]
    }
   ],
   "source": [
    "# exemple argmax\n",
    "a = np.arange(6).reshape(2,3) + 10\n",
    "print(\"a =\",a)\n",
    "print(\"axis=0 ->\",np.argmax(a,axis=0))  #axis=0 prends pour chaque colonne,l'indice du max\n",
    "print(\"axis=1 ->\",np.argmax(a,axis=1))  #axis=1 prends pour chaque ligne,l'indice du max "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28_18_10_2 reseau  0.01 learning rate,75 epochs,26 batchs -> 88%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-f99017df837f>:15: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/yaniv/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/yaniv/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "0 Batch accuracy: 0.692307710647583 Loss: 0.7229865193367004 Train_Accuracy: 0.5809248685836792\n",
      "1 Batch accuracy: 0.692307710647583 Loss: 0.6658163666725159 Train_Accuracy: 0.6315028667449951\n",
      "2 Batch accuracy: 0.7692307829856873 Loss: 0.6371657252311707 Train_Accuracy: 0.6791907548904419\n",
      "3 Batch accuracy: 0.8461538553237915 Loss: 0.612127959728241 Train_Accuracy: 0.7124277353286743\n",
      "4 Batch accuracy: 0.807692289352417 Loss: 0.5889171361923218 Train_Accuracy: 0.7413294911384583\n",
      "5 Batch accuracy: 0.692307710647583 Loss: 0.5644799470901489 Train_Accuracy: 0.7687861323356628\n",
      "6 Batch accuracy: 0.7307692170143127 Loss: 0.5397977232933044 Train_Accuracy: 0.7846820950508118\n",
      "7 Batch accuracy: 0.6538461446762085 Loss: 0.5176363587379456 Train_Accuracy: 0.7991329431533813\n",
      "8 Batch accuracy: 0.8846153616905212 Loss: 0.4983304440975189 Train_Accuracy: 0.810693621635437\n",
      "9 Batch accuracy: 0.692307710647583 Loss: 0.4815281629562378 Train_Accuracy: 0.8193641901016235\n",
      "10 Batch accuracy: 0.6538461446762085 Loss: 0.466916561126709 Train_Accuracy: 0.823699414730072\n",
      "11 Batch accuracy: 0.8846153616905212 Loss: 0.45434534549713135 Train_Accuracy: 0.8323699235916138\n",
      "12 Batch accuracy: 0.9615384340286255 Loss: 0.4434487223625183 Train_Accuracy: 0.8381502628326416\n",
      "13 Batch accuracy: 0.8461538553237915 Loss: 0.4336879551410675 Train_Accuracy: 0.839595377445221\n",
      "14 Batch accuracy: 0.8461538553237915 Loss: 0.4255252778530121 Train_Accuracy: 0.8453757166862488\n",
      "15 Batch accuracy: 0.9230769276618958 Loss: 0.417971134185791 Train_Accuracy: 0.8482658863067627\n",
      "16 Batch accuracy: 0.807692289352417 Loss: 0.4108133018016815 Train_Accuracy: 0.849711000919342\n",
      "17 Batch accuracy: 0.807692289352417 Loss: 0.40402600169181824 Train_Accuracy: 0.8511560559272766\n",
      "18 Batch accuracy: 0.8846153616905212 Loss: 0.3980219066143036 Train_Accuracy: 0.852601170539856\n",
      "19 Batch accuracy: 0.8461538553237915 Loss: 0.3931152820587158 Train_Accuracy: 0.852601170539856\n",
      "20 Batch accuracy: 0.9230769276618958 Loss: 0.3888453543186188 Train_Accuracy: 0.8540462255477905\n",
      "21 Batch accuracy: 0.8846153616905212 Loss: 0.3850463628768921 Train_Accuracy: 0.8554913401603699\n",
      "22 Batch accuracy: 0.8461538553237915 Loss: 0.38156893849372864 Train_Accuracy: 0.8554913401603699\n",
      "23 Batch accuracy: 0.8461538553237915 Loss: 0.37838295102119446 Train_Accuracy: 0.8554913401603699\n",
      "24 Batch accuracy: 0.8846153616905212 Loss: 0.37532785534858704 Train_Accuracy: 0.8540462255477905\n",
      "25 Batch accuracy: 0.8846153616905212 Loss: 0.372151643037796 Train_Accuracy: 0.8554913401603699\n",
      "26 Batch accuracy: 0.807692289352417 Loss: 0.36944085359573364 Train_Accuracy: 0.8569363951683044\n",
      "27 Batch accuracy: 0.7692307829856873 Loss: 0.36694127321243286 Train_Accuracy: 0.8598265647888184\n",
      "28 Batch accuracy: 0.8461538553237915 Loss: 0.3644789755344391 Train_Accuracy: 0.8598265647888184\n",
      "29 Batch accuracy: 0.8461538553237915 Loss: 0.3622133433818817 Train_Accuracy: 0.8598265647888184\n",
      "30 Batch accuracy: 0.8846153616905212 Loss: 0.36011314392089844 Train_Accuracy: 0.8612716794013977\n",
      "31 Batch accuracy: 0.9230769276618958 Loss: 0.35815298557281494 Train_Accuracy: 0.8627167344093323\n",
      "32 Batch accuracy: 0.8461538553237915 Loss: 0.35633614659309387 Train_Accuracy: 0.8627167344093323\n",
      "33 Batch accuracy: 0.7692307829856873 Loss: 0.3546164631843567 Train_Accuracy: 0.8627167344093323\n",
      "34 Batch accuracy: 1.0 Loss: 0.3527437448501587 Train_Accuracy: 0.8641618490219116\n",
      "35 Batch accuracy: 0.7307692170143127 Loss: 0.3510075509548187 Train_Accuracy: 0.8641618490219116\n",
      "36 Batch accuracy: 0.7307692170143127 Loss: 0.34935733675956726 Train_Accuracy: 0.8641618490219116\n",
      "37 Batch accuracy: 0.7692307829856873 Loss: 0.3478098213672638 Train_Accuracy: 0.8641618490219116\n",
      "38 Batch accuracy: 0.9230769276618958 Loss: 0.3462975025177002 Train_Accuracy: 0.8641618490219116\n",
      "39 Batch accuracy: 0.807692289352417 Loss: 0.34486037492752075 Train_Accuracy: 0.8641618490219116\n",
      "40 Batch accuracy: 0.7692307829856873 Loss: 0.3434807360172272 Train_Accuracy: 0.8641618490219116\n",
      "41 Batch accuracy: 0.9615384340286255 Loss: 0.3421570956707001 Train_Accuracy: 0.8641618490219116\n",
      "42 Batch accuracy: 0.807692289352417 Loss: 0.3408604562282562 Train_Accuracy: 0.8641618490219116\n",
      "43 Batch accuracy: 1.0 Loss: 0.3395412266254425 Train_Accuracy: 0.865606963634491\n",
      "44 Batch accuracy: 0.6538461446762085 Loss: 0.33817538619041443 Train_Accuracy: 0.8670520186424255\n",
      "45 Batch accuracy: 0.9230769276618958 Loss: 0.33687272667884827 Train_Accuracy: 0.8670520186424255\n",
      "46 Batch accuracy: 0.9615384340286255 Loss: 0.335552453994751 Train_Accuracy: 0.8670520186424255\n",
      "47 Batch accuracy: 0.8846153616905212 Loss: 0.3343070447444916 Train_Accuracy: 0.8684971332550049\n",
      "48 Batch accuracy: 0.8461538553237915 Loss: 0.33311060070991516 Train_Accuracy: 0.8684971332550049\n",
      "49 Batch accuracy: 0.8846153616905212 Loss: 0.33194008469581604 Train_Accuracy: 0.8684971332550049\n",
      "50 Batch accuracy: 0.7692307829856873 Loss: 0.33083391189575195 Train_Accuracy: 0.8684971332550049\n",
      "51 Batch accuracy: 0.8846153616905212 Loss: 0.3296630382537842 Train_Accuracy: 0.8684971332550049\n",
      "52 Batch accuracy: 0.692307710647583 Loss: 0.32838085293769836 Train_Accuracy: 0.8684971332550049\n",
      "53 Batch accuracy: 0.9230769276618958 Loss: 0.3270754814147949 Train_Accuracy: 0.8684971332550049\n",
      "54 Batch accuracy: 0.7692307829856873 Loss: 0.3257392346858978 Train_Accuracy: 0.8684971332550049\n",
      "55 Batch accuracy: 0.8846153616905212 Loss: 0.32431915402412415 Train_Accuracy: 0.8684971332550049\n",
      "56 Batch accuracy: 0.8461538553237915 Loss: 0.32300928235054016 Train_Accuracy: 0.8684971332550049\n",
      "57 Batch accuracy: 0.7307692170143127 Loss: 0.3218008279800415 Train_Accuracy: 0.8684971332550049\n",
      "58 Batch accuracy: 0.8846153616905212 Loss: 0.3206250071525574 Train_Accuracy: 0.8684971332550049\n",
      "59 Batch accuracy: 1.0 Loss: 0.3194528818130493 Train_Accuracy: 0.8684971332550049\n",
      "60 Batch accuracy: 0.8461538553237915 Loss: 0.31827351450920105 Train_Accuracy: 0.8684971332550049\n",
      "61 Batch accuracy: 0.8461538553237915 Loss: 0.3170725107192993 Train_Accuracy: 0.8684971332550049\n",
      "62 Batch accuracy: 0.8846153616905212 Loss: 0.31579554080963135 Train_Accuracy: 0.8684971332550049\n",
      "63 Batch accuracy: 0.8846153616905212 Loss: 0.3144230544567108 Train_Accuracy: 0.8684971332550049\n",
      "64 Batch accuracy: 0.807692289352417 Loss: 0.3130556643009186 Train_Accuracy: 0.8699421882629395\n",
      "65 Batch accuracy: 0.807692289352417 Loss: 0.31165432929992676 Train_Accuracy: 0.8699421882629395\n",
      "66 Batch accuracy: 0.8461538553237915 Loss: 0.3104036748409271 Train_Accuracy: 0.8699421882629395\n",
      "67 Batch accuracy: 0.8846153616905212 Loss: 0.30922508239746094 Train_Accuracy: 0.8699421882629395\n",
      "68 Batch accuracy: 0.8846153616905212 Loss: 0.3080260455608368 Train_Accuracy: 0.8699421882629395\n",
      "69 Batch accuracy: 0.7692307829856873 Loss: 0.30679643154144287 Train_Accuracy: 0.8699421882629395\n",
      "70 Batch accuracy: 0.9615384340286255 Loss: 0.305605411529541 Train_Accuracy: 0.8713873028755188\n",
      "71 Batch accuracy: 0.8846153616905212 Loss: 0.30434635281562805 Train_Accuracy: 0.8713873028755188\n",
      "72 Batch accuracy: 0.9230769276618958 Loss: 0.3030489385128021 Train_Accuracy: 0.8713873028755188\n",
      "73 Batch accuracy: 0.9230769276618958 Loss: 0.30181172490119934 Train_Accuracy: 0.8713873028755188\n",
      "74 Batch accuracy: 0.8846153616905212 Loss: 0.3006279468536377 Train_Accuracy: 0.8713873028755188\n",
      "WARNING:tensorflow:From /home/yaniv/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "Predicted classes: [1 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 1 0 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 0 0]\n",
      "Actual classes:    [1 1 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0 1\n",
      " 1 0 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0]\n",
      "accuracy_score :  0.8823529411764706\n",
      "matthews_corrcoef :  0.7628596338273758\n",
      "F1_score :  0.8709677419354839\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn\n",
    "n_inputs = 28    # effet entonnoir , quelques liens pour comprendre comment choisir \n",
    "n_hidden1 = 18   # le nb de neurons par couches : https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "n_hidden2 = 10   # https://www.researchgate.net/post/How_to_decide_the_number_of_hidden_layers_and_nodes_in_a_hidden_layer\n",
    "n_outputs = 2    # 2 classes , 2 outputs\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")   #permets d'utiliser X sans lui rentrer tout de suite ses donnees\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")   #None => any\n",
    "\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",     #prend en entrée nos données(phrase), puisque X sera nourrit dans le feed_dict par X_train\n",
    "                               activation=tf.nn.relu)       #et en sortie le nb de neurones dans la 2e couche\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", # prend en entrée la couche precedente etc...\n",
    "                               activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")  #derniere couche, donc renvoie les valeurs sorties par le reseau\n",
    "\n",
    "learning_rate =0.01    \n",
    "\n",
    "\n",
    "#le reseau est pret , on definit la fonction de cout, et pour ca on utilise l'entropie croisée\n",
    "xentropy = tf.keras.backend.binary_crossentropy(tf.cast(y,tf.float32),logits)  # penalise les faible proba , on utilise une pour binary classificaion ici \n",
    "loss = tf.reduce_mean(xentropy)                                                #  l'entroprie est calculée à partir des logits et attend les labels de y , reduce_mean est l'entroprie croisée moyenne sur toutes les instances\n",
    "\n",
    "#ajuste les parametres du modele pour minimiser la fonction cout\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "#comment on évalue le modele,ici l'exactitude , on va verifier si la prevision est correcte en verifiant si le logit le plus eleve = le bon label\n",
    "correct = tf.nn.in_top_k(logits, tf.argmax(y, 1), 1) #voir exemple dans la cellule d'apres pour cette fonction\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))  #moyenne , donc precision gloable du modele\n",
    "\n",
    "init = tf.global_variables_initializer()  #noeud qui initialise toutes les variables\n",
    "saver = tf.train.Saver()                  #pour enregistrer le modele\n",
    "n_epochs = 75                           \n",
    "batch_size = 26\n",
    "\n",
    "#phase d'execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()                            #execute le noeud init\n",
    "    \n",
    "    for epoch in range(n_epochs):         # a chaque epoch, on evalue chaque  minis paquets qui correspondent au X_train a la fin de la sous boucle\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, Y_train, batch_size):  #pour chaque mini-lot choisis de maniere homogene dans X_train,Y_train\n",
    "                _, cost, corr, acc = sess.run([training_op, loss, correct, accuracy], feed_dict={X: X_train, y: to_one_hot(Y_train)})  #on execute l'operation d'entrainement sur training_op et sur d'autres parametres pour avoir une meilleure idée de ce qu'il se passe (cf print suivant),\n",
    "        \n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: to_one_hot(y_batch)})               #a la fin de chaque etape, le code evalue l'exactitude du modele sur le dernier mini lot (car hors boucle) \n",
    "        print(epoch,'Batch accuracy: {} Loss: {} Train_Accuracy: {}'.format(acc_batch,cost, acc))\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "#le modele est entrainé, on test\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")  # restore le model\n",
    "    Z = logits.eval(feed_dict={X: X_test})        # evalue la couche de sortie sur X_test\n",
    "    y_pred = np.argmax(Z, axis=1)                 #renvoie l'indice du max de chaque ligne (axis=1) de Z , donc la proba la plus elevée parmis les classes\n",
    "    \n",
    "    print(\"Predicted classes:\",y_pred)\n",
    "    print(\"Actual classes:   \", Y_test)\n",
    "    print(\"accuracy_score : \",sklearn.metrics.accuracy_score(Y_test, y_pred, normalize=True ,sample_weight=None))\n",
    "    print(\"matthews_corrcoef : \",sklearn.metrics.matthews_corrcoef(Y_test, y_pred, sample_weight=None))\n",
    "    print(\"F1_score : \",sklearn.metrics.f1_score(Y_test, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exemple de tf.nn.in_top_k(logits, y, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retourne le booleen qui dit si le max de logits[i] se trouve effectivement à colonne y[i]\n",
      "  ici, est ce que le max de logits[0] = [0,1] se trouve à la colonne y[0]=0 ? --> False\n",
      "  ici, est ce que le max de logits[2] = [0,1] se trouve à la colonne y[2]=1 ? --> true\n",
      "\n",
      "[False False  True False  True]\n"
     ]
    }
   ],
   "source": [
    "# logits est un tenseur de type float32 de shape (batch_size,n_classes)\n",
    "# y est un tenseur de type int32 de shape (batch_size) , c'est les classes_ids\n",
    "# k est le nb d'elements à prendre (precision)\n",
    "#retourne un array de booleen\n",
    "test = tf.nn.in_top_k([[0,1], [1,0], [0,1], [1, 0], [0, 1]], [0, 1, 1, 1, 1], 1) #logits_shape=(5,2) et y_shape=(5,)\n",
    "with tf.Session() as sess:\n",
    "    print(\"retourne le booleen qui dit si le max de logits[i] se trouve effectivement à colonne y[i]\")\n",
    "    print(\" \",\"ici, est ce que le max de logits[0] = [0,1] se trouve à la colonne y[0]=0 ? --> False\")\n",
    "    print(\" \",\"ici, est ce que le max de logits[2] = [0,1] se trouve à la colonne y[2]=1 ? --> true\")\n",
    "    print()\n",
    "    print(test.eval())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
