{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"training_linear_models\"\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)   \n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)   #affichage non tronqué"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recupere BDD -> Matrices d'inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ROWS=1293\n",
    "\n",
    "#recupere les données du fichier csv et les mets dans les ndarray X et Y (grace à to_numpy(),sinon ce sont des types dataframe)\n",
    "#Ils ont donc une shape de (671,1)\n",
    "\n",
    "X= pd.read_csv(\"sentence_1293.csv\", encoding = \"utf-8\",sep=\";\",usecols = [\"sen_sentence\"],nrows=ROWS).to_numpy()\n",
    "Y= pd.read_csv(\"sentence_1293.csv\", encoding = \"utf-8\",sep=\";\",usecols = [\"sen_en_full\"],nrows=ROWS).to_numpy().reshape(-1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype('int64')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(Y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint                            #permet un affichage ligne par ligne,équivalent au println() de java\n",
    "from nltk.tag import StanfordPOSTagger   #librairie utilisée pour tagger \n",
    "\n",
    "jar = './stanford-postagger-full-2018-10-16/stanford-postagger.jar'    # recupere les modeles telechargés dans le fichier\n",
    "model = './stanford-postagger-full-2018-10-16/models/french.tagger'\n",
    "java_path = \"C:/Program Files/Java/jre1.8.0_211/bin/java.exe\"          # on s'assure du lien pour java\n",
    "os.environ['JAVAHOME'] = java_path                                     # variable d'environnement\n",
    "\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding='utf-8' )          #prêt à être utilisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonctions qui transforme une phrase en matrice lignes (taille 28) de nos inputs , binarisation des tags\n",
    "\n",
    "def senToTags(sentence):\n",
    "    sen_tags=[m[1] for m in pos_tagger.tag(sentence)]        #tout les tags presents dans la phrase\n",
    "    #dictionnaire des tags possibles (28)\n",
    "    tags=['P', 'N', 'ADJ', 'NC', 'DET', 'NPP', 'V', 'VPP', 'ADV', 'PROREL', 'CLS', 'VINF', 'CC', 'PUNC', 'PRO', 'ET', 'CS', 'CLR', 'CLO', 'VPR', 'ADVWH', 'C', 'VIMP', 'CL', 'VS', 'PROWH', 'ADJWH', 'PREF']\n",
    "    result=list()\n",
    "    for tag in tags:            # pour chaque tag du dictionnaire\n",
    "        if tag in sen_tags:     # on vérifie s'il est présent dans la phrase\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exemple : senToTags(\" Les Americains ont réagi en six semaines là où il a fallu neuf mois aux Francais.\") --->  [1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"exemple : senToTags(\\\" Les Americains ont réagi en six semaines là où il a fallu neuf mois aux Francais.\\\") ---> \",senToTags(\"Les Americains ont réagi en six semaines là où il a fallu neuf mois aux Francais.\"))\n",
    "print(senToTags(\"Je\"))\n",
    "print(senToTags(\"suis\"))\n",
    "print(senToTags(\"beau\"))\n",
    "print(senToTags(\"Je suis beau\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time execution :  2220.243691444397\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()          # recupere l'heure exacte pour calculer le temps d'execution\n",
    "X_tmp=list()                 # initialise une list\n",
    "for i in range(len(X)):\n",
    "    X_tmp+=senToTags(X[i])  #on applique cette fonction (cellule precedente) à chaque phrase, à chaque ligne de X donc\n",
    "end = time.time()\n",
    "print(\"time execution : \",end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of label '1': 610\n",
      "Before OverSampling, counts of label '0': 553 \n",
      "\n",
      "After OverSampling, the shape of train_X: (1220, 28)\n",
      "After OverSampling, the shape of train_y: (1220,) \n",
      "\n",
      "After OverSampling, counts of label '1': 610\n",
      "After OverSampling, counts of label '0': 610\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tmp=np.asarray(X_tmp).reshape(-1,28)  #redimensionne en imposant 28 colonnes , peut importe le nb de lignes\n",
    "#simple separation de X_tmp en X_train et X_test selon le ratio , test_size ... meme chose pour Y\n",
    "# le random_state=2 permet uniquement de fixer l'aleatoire apres un premier tirage pour ne pas avoir de problemes en reexecutant \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tmp, Y, test_size=0.1, random_state=2)\n",
    "\n",
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(Y_train==1)))    # nb de label=1 avant equilibrage\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(Y_train==0))) # nb de label=0 avant equilibrage\n",
    "\n",
    "\n",
    "sm = SMOTE(random_state=2)                       #initialise le regulateur qui va en realité crée des doublons de la classe\n",
    "X_train, Y_train = sm.fit_sample(X_train, Y_train)  # inferieur pour egaliser le nb de labels\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train.shape)) \n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(Y_train.shape)) \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(Y_train == 1))) \n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(Y_train == 0))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions pour le reseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#couche de neurones, on aurait pu utilisé tf.layers.dense() qui prend les meme parametres et qui crée une couche intégralement connectée, elle se charge de creer les var necessaires en utilisant la stratégie d'initialisationj appropriée\n",
    "\n",
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name): #portée de nom , facultatif, pour mieux retrouver\n",
    "        n_inputs = int(X.shape[1])  #nb entrées = nb de colonnes de X donc 28 \n",
    "        \n",
    "        #ces 3 lignes créent la var W qui contiendra la matrice des poids des connexions entre chaque entrée et chaque neurone\n",
    "        stddev = 2 / np.sqrt(n_inputs)  #facultatif, on tronquera avec cet ecart-type qui permet à l'algo de converger plus rapidement,plus stddev est grand et plus l'ecart sera grand\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev) #matrice n_inputs x n_neurons valeurs proche de 0 , tronquées avec stddeb   \n",
    "        W = tf.Variable(init,name=\"kernel\")  #weights random\n",
    "        \n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\") #biais,initialisé à 0\n",
    "        Z = tf.matmul(X, W) + b    #z=X.W +b\n",
    "        Z = tf.cast(Z,tf.int32)\n",
    "        if activation is not None:  #passe par une fonction d'activation si elle existe\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z\n",
    "\n",
    "           \n",
    "        \n",
    "#return un Y avec une dimension = valeur max+1 , donc ici y_one_hot aura une shape=(any,2)\n",
    "#voir exemple cellule d'apres\n",
    "\n",
    "def to_one_hot(y):\n",
    "    n_classes = y.max() + 1  # dimension +1 dans notre cas , ce sera forcement 2\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m, n_classes)) #matrice de dimension(m,n_classes) remplit de 0\n",
    "    Y_one_hot[np.arange(m), y] = 1  #on mets un 1 dans la colonne 1 etc.. voir exemple\n",
    "    return Y_one_hot\n",
    "\n",
    "\n",
    "#Homogeneisation, decoupage de X et Y en paquets de taille = batch_size, choisis aléatoirement dans X et Y , permets d'eviter\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):               #de calculer que sur une série de données \"particuliere\" comme l'interview de Rihanna et pas sur le reste, ici au moins , on a des données homogenes\n",
    "    rnd_idx = np.random.permutation(len(X))              #array de taille = 671  de valeur aleatoires < 671  \n",
    "    n_batches = len(X) // batch_size               #nb de paquets\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):  #np.array_split(X,n) split X en n parties(array)\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]     #on declare nos paquets \n",
    "        yield X_batch, y_batch                           #yield est comme un return sauf que c'est un generateur , ce qui permet de gagner en memoire , il ne parcourt pas à chaque appel toute la fonction mais recupere à la volée la valeur puis incremente ensuite le generateur , un lien qui explique davanatage : https://www.journaldunet.fr/web-tech/developpement/1202863-a-quoi-sert-le-mot-cle-yield-en-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exemples fonctions utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_shape =  (6,)\n",
      "y1_shape =  (6, 2)\n",
      "y1 = [[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#exemple y_one_hot\n",
    "\n",
    "y=np.asarray([1,0,1,1,0,0])\n",
    "\n",
    "print(\"y_shape = \",y.shape)\n",
    "y1=to_one_hot(y)\n",
    "print(\"y1_shape = \",y1.shape)\n",
    "\n",
    "#on mets pour chaque element de y ayant pour valeur i , un 1 à la colonne i et sur le reste de la ligne, que des 0 \n",
    "print(\"y1 =\" ,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20314875  0.01988505]\n",
      " [-0.09518349 -0.11978555]\n",
      " [ 0.14485252  0.0532154 ]]\n"
     ]
    }
   ],
   "source": [
    "#exemple truncated_normal\n",
    "\n",
    "import tensorflow as tf\n",
    "with  tf.Session() as sess:\n",
    "    p=tf.truncated_normal((3,2),stddev=0.2).eval()\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [[10 11 12]\n",
      " [13 14 15]]\n",
      "axis=0 -> [1 1 1]\n",
      "axis=1 -> [2 2]\n"
     ]
    }
   ],
   "source": [
    "# exemple argmax\n",
    "a = np.arange(6).reshape(2,3) + 10\n",
    "print(\"a =\",a)\n",
    "print(\"axis=0 ->\",np.argmax(a,axis=0))  #axis=0 prends pour chaque colonne,l'indice du max\n",
    "print(\"axis=1 ->\",np.argmax(a,axis=1))  #axis=1 prends pour chaque ligne,l'indice du max "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28_18_11_2 reseau  0.01 learning rate,75 epochs,26 batchs -> 91%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-bb52d814efda>:15: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/yaniv/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/yaniv/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "0 Batch accuracy: 0.5384615659713745 Loss: 0.6516870260238647 Train_Accuracy: 0.6213114857673645\n",
      "1 Batch accuracy: 0.7307692170143127 Loss: 0.5982598066329956 Train_Accuracy: 0.6975409984588623\n",
      "2 Batch accuracy: 0.7307692170143127 Loss: 0.5531545281410217 Train_Accuracy: 0.7442622780799866\n",
      "3 Batch accuracy: 0.7692307829856873 Loss: 0.5144656300544739 Train_Accuracy: 0.7795081734657288\n",
      "4 Batch accuracy: 0.807692289352417 Loss: 0.48532843589782715 Train_Accuracy: 0.8049180507659912\n",
      "5 Batch accuracy: 0.8461538553237915 Loss: 0.46316108107566833 Train_Accuracy: 0.8204917907714844\n",
      "6 Batch accuracy: 0.9615384340286255 Loss: 0.4460398256778717 Train_Accuracy: 0.8262295126914978\n",
      "7 Batch accuracy: 0.7692307829856873 Loss: 0.4316900670528412 Train_Accuracy: 0.8393442630767822\n",
      "8 Batch accuracy: 0.8846153616905212 Loss: 0.41968193650245667 Train_Accuracy: 0.8434426188468933\n",
      "9 Batch accuracy: 0.7307692170143127 Loss: 0.4097774922847748 Train_Accuracy: 0.8434426188468933\n",
      "10 Batch accuracy: 0.8846153616905212 Loss: 0.4014088213443756 Train_Accuracy: 0.8475409746170044\n",
      "11 Batch accuracy: 0.8461538553237915 Loss: 0.39415931701660156 Train_Accuracy: 0.8508196473121643\n",
      "12 Batch accuracy: 0.8461538553237915 Loss: 0.38784223794937134 Train_Accuracy: 0.8500000238418579\n",
      "13 Batch accuracy: 0.9615384340286255 Loss: 0.3829469084739685 Train_Accuracy: 0.854098379611969\n",
      "14 Batch accuracy: 0.9615384340286255 Loss: 0.37892138957977295 Train_Accuracy: 0.8532786965370178\n",
      "15 Batch accuracy: 0.7692307829856873 Loss: 0.37563422322273254 Train_Accuracy: 0.8549180030822754\n",
      "16 Batch accuracy: 0.807692289352417 Loss: 0.3728090226650238 Train_Accuracy: 0.8557376861572266\n",
      "17 Batch accuracy: 0.7692307829856873 Loss: 0.37032026052474976 Train_Accuracy: 0.8573770523071289\n",
      "18 Batch accuracy: 0.9230769276618958 Loss: 0.36815041303634644 Train_Accuracy: 0.8573770523071289\n",
      "19 Batch accuracy: 0.8846153616905212 Loss: 0.3662404417991638 Train_Accuracy: 0.8573770523071289\n",
      "20 Batch accuracy: 0.9230769276618958 Loss: 0.3645246624946594 Train_Accuracy: 0.8573770523071289\n",
      "21 Batch accuracy: 0.7692307829856873 Loss: 0.36291059851646423 Train_Accuracy: 0.8573770523071289\n",
      "22 Batch accuracy: 0.9615384340286255 Loss: 0.3614101707935333 Train_Accuracy: 0.8573770523071289\n",
      "23 Batch accuracy: 0.8461538553237915 Loss: 0.3599766194820404 Train_Accuracy: 0.8573770523071289\n",
      "24 Batch accuracy: 0.9230769276618958 Loss: 0.3585716485977173 Train_Accuracy: 0.8573770523071289\n",
      "25 Batch accuracy: 0.8846153616905212 Loss: 0.357231467962265 Train_Accuracy: 0.8573770523071289\n",
      "26 Batch accuracy: 0.9230769276618958 Loss: 0.3559054136276245 Train_Accuracy: 0.8573770523071289\n",
      "27 Batch accuracy: 0.9230769276618958 Loss: 0.35465604066848755 Train_Accuracy: 0.8573770523071289\n",
      "28 Batch accuracy: 0.7692307829856873 Loss: 0.35346275568008423 Train_Accuracy: 0.8573770523071289\n",
      "29 Batch accuracy: 0.9230769276618958 Loss: 0.3523232340812683 Train_Accuracy: 0.8565573692321777\n",
      "30 Batch accuracy: 0.7307692170143127 Loss: 0.3512333333492279 Train_Accuracy: 0.8565573692321777\n",
      "31 Batch accuracy: 0.9230769276618958 Loss: 0.35024890303611755 Train_Accuracy: 0.8565573692321777\n",
      "32 Batch accuracy: 0.8461538553237915 Loss: 0.34933650493621826 Train_Accuracy: 0.8565573692321777\n",
      "33 Batch accuracy: 0.9615384340286255 Loss: 0.3484630286693573 Train_Accuracy: 0.8573770523071289\n",
      "34 Batch accuracy: 0.8461538553237915 Loss: 0.34762173891067505 Train_Accuracy: 0.8581967353820801\n",
      "35 Batch accuracy: 0.9230769276618958 Loss: 0.3467845618724823 Train_Accuracy: 0.8573770523071289\n",
      "36 Batch accuracy: 0.807692289352417 Loss: 0.34598836302757263 Train_Accuracy: 0.8573770523071289\n",
      "37 Batch accuracy: 0.9230769276618958 Loss: 0.3451789319515228 Train_Accuracy: 0.8581967353820801\n",
      "38 Batch accuracy: 0.8846153616905212 Loss: 0.3443928062915802 Train_Accuracy: 0.8581967353820801\n",
      "39 Batch accuracy: 0.8461538553237915 Loss: 0.3436150550842285 Train_Accuracy: 0.8581967353820801\n",
      "40 Batch accuracy: 0.9615384340286255 Loss: 0.34286874532699585 Train_Accuracy: 0.8573770523071289\n",
      "41 Batch accuracy: 0.8461538553237915 Loss: 0.34215161204338074 Train_Accuracy: 0.8565573692321777\n",
      "42 Batch accuracy: 0.807692289352417 Loss: 0.3414512872695923 Train_Accuracy: 0.8573770523071289\n",
      "43 Batch accuracy: 0.7692307829856873 Loss: 0.3407735228538513 Train_Accuracy: 0.8573770523071289\n",
      "44 Batch accuracy: 0.8846153616905212 Loss: 0.3401103913784027 Train_Accuracy: 0.8573770523071289\n",
      "45 Batch accuracy: 1.0 Loss: 0.33940014243125916 Train_Accuracy: 0.8573770523071289\n",
      "46 Batch accuracy: 0.9615384340286255 Loss: 0.33866095542907715 Train_Accuracy: 0.8581967353820801\n",
      "47 Batch accuracy: 0.9230769276618958 Loss: 0.33796143531799316 Train_Accuracy: 0.8598360419273376\n",
      "48 Batch accuracy: 0.8461538553237915 Loss: 0.33728814125061035 Train_Accuracy: 0.8598360419273376\n",
      "49 Batch accuracy: 0.9230769276618958 Loss: 0.3366228938102722 Train_Accuracy: 0.8598360419273376\n",
      "50 Batch accuracy: 0.8461538553237915 Loss: 0.33598586916923523 Train_Accuracy: 0.8598360419273376\n",
      "51 Batch accuracy: 0.8461538553237915 Loss: 0.3353661000728607 Train_Accuracy: 0.8598360419273376\n",
      "52 Batch accuracy: 0.807692289352417 Loss: 0.33475616574287415 Train_Accuracy: 0.8598360419273376\n",
      "53 Batch accuracy: 0.8846153616905212 Loss: 0.3341434597969055 Train_Accuracy: 0.8598360419273376\n",
      "54 Batch accuracy: 0.7692307829856873 Loss: 0.3335452675819397 Train_Accuracy: 0.8606557250022888\n",
      "55 Batch accuracy: 0.8846153616905212 Loss: 0.3329615294933319 Train_Accuracy: 0.8606557250022888\n",
      "56 Batch accuracy: 0.9230769276618958 Loss: 0.33240196108818054 Train_Accuracy: 0.8606557250022888\n",
      "57 Batch accuracy: 0.7692307829856873 Loss: 0.33185797929763794 Train_Accuracy: 0.8606557250022888\n",
      "58 Batch accuracy: 0.807692289352417 Loss: 0.33132413029670715 Train_Accuracy: 0.8606557250022888\n",
      "59 Batch accuracy: 0.807692289352417 Loss: 0.33079054951667786 Train_Accuracy: 0.8606557250022888\n",
      "60 Batch accuracy: 0.8461538553237915 Loss: 0.3302666246891022 Train_Accuracy: 0.8606557250022888\n",
      "61 Batch accuracy: 0.9230769276618958 Loss: 0.3297377824783325 Train_Accuracy: 0.8606557250022888\n",
      "62 Batch accuracy: 0.9230769276618958 Loss: 0.32918140292167664 Train_Accuracy: 0.86147540807724\n",
      "63 Batch accuracy: 0.9230769276618958 Loss: 0.3286372423171997 Train_Accuracy: 0.86147540807724\n",
      "64 Batch accuracy: 0.807692289352417 Loss: 0.3281061053276062 Train_Accuracy: 0.86147540807724\n",
      "65 Batch accuracy: 0.8461538553237915 Loss: 0.3275797367095947 Train_Accuracy: 0.86147540807724\n",
      "66 Batch accuracy: 0.9230769276618958 Loss: 0.32706424593925476 Train_Accuracy: 0.86147540807724\n",
      "67 Batch accuracy: 0.8461538553237915 Loss: 0.3265504837036133 Train_Accuracy: 0.86147540807724\n",
      "68 Batch accuracy: 0.8846153616905212 Loss: 0.32602864503860474 Train_Accuracy: 0.8622950911521912\n",
      "69 Batch accuracy: 0.8846153616905212 Loss: 0.3255085051059723 Train_Accuracy: 0.8631147742271423\n",
      "70 Batch accuracy: 0.8846153616905212 Loss: 0.3249756991863251 Train_Accuracy: 0.8631147742271423\n",
      "71 Batch accuracy: 0.807692289352417 Loss: 0.3244386911392212 Train_Accuracy: 0.8639343976974487\n",
      "72 Batch accuracy: 0.8846153616905212 Loss: 0.32391226291656494 Train_Accuracy: 0.8647540807723999\n",
      "73 Batch accuracy: 0.807692289352417 Loss: 0.32338666915893555 Train_Accuracy: 0.8663934469223022\n",
      "74 Batch accuracy: 0.7307692170143127 Loss: 0.3228589594364166 Train_Accuracy: 0.8655737638473511\n",
      "WARNING:tensorflow:From /home/yaniv/.local/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "Predicted classes: [0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 0 0 1 1\n",
      " 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1\n",
      " 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 1 0 0 0 1\n",
      " 1 1 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1]\n",
      "Actual classes:    [0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 1 1\n",
      " 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 1\n",
      " 0 1 1 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1\n",
      " 1 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1]\n",
      "accuracy_score :  0.9153846153846154\n",
      "matthews_corrcoef :  0.8317348067729966\n",
      "F1_score :  0.9160305343511451\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn\n",
    "n_inputs = 28    # effet entonnoir , quelques liens pour comprendre comment choisir \n",
    "n_hidden1 = 18   # le nb de neurons par couches : https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n",
    "n_hidden2 = 11   # https://www.researchgate.net/post/How_to_decide_the_number_of_hidden_layers_and_nodes_in_a_hidden_layer\n",
    "n_outputs = 2    # 2 classes , 2 outputs\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")   #permets d'utiliser X sans lui rentrer tout de suite ses donnees\n",
    "y = tf.placeholder(tf.float32, shape=(None), name=\"y\")   #None => any\n",
    "\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\",     #prend en entrée nos données(phrase), puisque X sera nourrit dans le feed_dict par X_train\n",
    "                               activation=tf.nn.relu)       #et en sortie le nb de neurones dans la 2e couche\n",
    "hidden2 = tf.layers.dense(hidden1, n_hidden2, name=\"hidden2\", # prend en entrée la couche precedente etc...\n",
    "                               activation=tf.nn.relu)\n",
    "logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")  #derniere couche, donc renvoie les valeurs sorties par le reseau\n",
    "\n",
    "learning_rate =0.01    \n",
    "\n",
    "\n",
    "#le reseau est pret , on definit la fonction de cout, et pour ca on utilise l'entropie croisée\n",
    "xentropy = tf.keras.backend.binary_crossentropy(y,logits)  # penalise les faible proba , on utilise une pour binary classificaion ici \n",
    "loss = tf.reduce_mean(xentropy)                                                #  l'entroprie est calculée à partir des logits et attend les labels de y , reduce_mean est l'entroprie croisée moyenne sur toutes les instances\n",
    "\n",
    "#ajuste les parametres du modele pour minimiser la fonction cout\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "#comment on évalue le modele,ici l'exactitude , on va verifier si la prevision est correcte en verifiant si le logit le plus eleve = le bon label\n",
    "correct = tf.nn.in_top_k(logits, tf.argmax(y, 1), 1) #voir exemple dans la cellule d'apres pour cette fonction\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))  #moyenne , donc precision gloable du modele\n",
    "\n",
    "init = tf.global_variables_initializer()  #noeud qui initialise toutes les variables\n",
    "saver = tf.train.Saver()                  #pour enregistrer le modele\n",
    "n_epochs = 75                           \n",
    "batch_size = 26\n",
    "\n",
    "#phase d'execution\n",
    "with tf.Session() as sess:\n",
    "    init.run()                            #execute le noeud init\n",
    "    \n",
    "    for epoch in range(n_epochs):         # a chaque epoch, on evalue chaque  minis paquets qui correspondent au X_train a la fin de la sous boucle\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, Y_train, batch_size):  #pour chaque mini-lot choisis de maniere homogene dans X_train,Y_train\n",
    "                _, cost, corr, acc = sess.run([training_op, loss, correct, accuracy], feed_dict={X: X_train, y: to_one_hot(Y_train)})  #on execute l'operation d'entrainement sur training_op et sur d'autres parametres pour avoir une meilleure idée de ce qu'il se passe (cf print suivant),\n",
    "        \n",
    "        acc_batch = accuracy.eval(feed_dict={X: X_batch, y: to_one_hot(y_batch)})               #a la fin de chaque etape, le code evalue l'exactitude du modele sur le dernier mini lot (car hors boucle) \n",
    "        print(epoch,'Batch accuracy: {} Loss: {} Train_Accuracy: {}'.format(acc_batch,cost, acc))\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "#le modele est entrainé, on test\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")  # restore le model\n",
    "    Z = logits.eval(feed_dict={X: X_test})        # evalue la couche de sortie sur X_test\n",
    "    y_pred = np.argmax(Z, axis=1)                 #renvoie l'indice du max de chaque ligne (axis=1) de Z , donc la proba la plus elevée parmis les classes\n",
    "    \n",
    "    print(\"Predicted classes:\",y_pred)\n",
    "    print(\"Actual classes:   \", Y_test)\n",
    "    print(\"accuracy_score : \",sklearn.metrics.accuracy_score(Y_test, y_pred, normalize=True ,sample_weight=None))\n",
    "    print(\"matthews_corrcoef : \",sklearn.metrics.matthews_corrcoef(Y_test, y_pred, sample_weight=None))\n",
    "    print(\"F1_score : \",sklearn.metrics.f1_score(Y_test, y_pred, labels=None, pos_label=1, average='binary', sample_weight=None))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exemple de tf.nn.in_top_k(logits, y, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retourne le booleen qui dit si le max de logits[i] se trouve effectivement à colonne y[i]\n",
      "  ici, est ce que le max de logits[0] = [0,1] se trouve à la colonne y[0]=0 ? --> False\n",
      "  ici, est ce que le max de logits[2] = [0,1] se trouve à la colonne y[2]=1 ? --> true\n",
      "\n",
      "[False False  True False  True]\n"
     ]
    }
   ],
   "source": [
    "# logits est un tenseur de type float32 de shape (batch_size,n_classes)\n",
    "# y est un tenseur de type int32 de shape (batch_size) , c'est les classes_ids\n",
    "# k est le nb d'elements à prendre (precision)\n",
    "#retourne un array de booleen\n",
    "test = tf.nn.in_top_k([[0,1], [1,0], [0,1], [1, 0], [0, 1]], [0, 1, 1, 1, 1], 1) #logits_shape=(5,2) et y_shape=(5,)\n",
    "with tf.Session() as sess:\n",
    "    print(\"retourne le booleen qui dit si le max de logits[i] se trouve effectivement à colonne y[i]\")\n",
    "    print(\" \",\"ici, est ce que le max de logits[0] = [0,1] se trouve à la colonne y[0]=0 ? --> False\")\n",
    "    print(\" \",\"ici, est ce que le max de logits[2] = [0,1] se trouve à la colonne y[2]=1 ? --> true\")\n",
    "    print()\n",
    "    print(test.eval())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
